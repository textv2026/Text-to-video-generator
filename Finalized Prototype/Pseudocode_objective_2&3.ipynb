{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm: Chinese Speech Recognition and Waveform Visualization\n",
        "\n",
        "## Input:\n",
        "- AudioFile: Input audio file path\n",
        "- APPID, APIKey, APISecret: API credentials\n",
        "- Sample rate: 16000 Hz\n",
        "- Frame size: 8000 bytes\n",
        "- Interval: 0.04 seconds\n",
        "\n",
        "## Output:\n",
        "- Word timings with timestamps\n",
        "- Visualized waveform with Pinyin annotations\n",
        "\n",
        "## Main Procedure:\n",
        "\n",
        "1. Initialize:\n",
        "   - word_timings = empty list\n",
        "   - Create WebSocket connection parameters\n",
        "   - Generate authentication URL using HMAC-SHA256\n",
        "\n",
        "2. ProcessAudioStream(AudioFile):\n",
        "   status = FIRST_FRAME\n",
        "   WHILE not end of file:\n",
        "       buffer = read frameSize bytes from AudioFile\n",
        "       IF buffer is empty:\n",
        "           status = LAST_FRAME\n",
        "       \n",
        "       encoded_audio = base64_encode(buffer)\n",
        "       \n",
        "       IF status == FIRST_FRAME:\n",
        "           send initial frame with config\n",
        "           status = CONTINUE_FRAME\n",
        "       ELSE IF status == CONTINUE_FRAME:\n",
        "           send continuation frame\n",
        "       ELSE:\n",
        "           send final frame\n",
        "           break\n",
        "       \n",
        "       wait(interval)\n",
        "\n",
        "3. HandleServerResponse(message):\n",
        "   IF message is valid:\n",
        "       FOR each word in message.result:\n",
        "           time = word.begin_time × 10\n",
        "           word_timings.append((time, word.text))\n",
        "\n",
        "4. VisualizeResults:\n",
        "   - Load audio file as numpy array\n",
        "   - Create time axis\n",
        "   - Plot waveform\n",
        "   FOR each (timestamp, word) in word_timings:\n",
        "       - Convert word to Pinyin\n",
        "       - Draw vertical line at timestamp\n",
        "       - Annotate with Pinyin text\n",
        "\n",
        "## Error Handling:\n",
        "- WebSocket connection errors\n",
        "- Invalid message format\n",
        "- Audio file reading errors\n",
        "- Authentication failures\n",
        "\n",
        "## Constraints:\n",
        "- Audio format: audio/L16\n",
        "- Sample rate: 16000 Hz\n",
        "- Language: Mandarin Chinese\n",
        "- VAD end silence: 10000ms"
      ],
      "metadata": {
        "id": "a0GH8DLd04se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm: Audio Pause Detection and Analysis\n",
        "\n",
        "## Input:\n",
        "- audio_file: Input audio signal\n",
        "- window_len: Smoothing window length (default: 1000)\n",
        "- min_silence_len: Minimum silence duration (ms) (default: 500)\n",
        "- debounce_time: Minimum time between pauses (ms) (default: 300)\n",
        "\n",
        "## Output:\n",
        "- List of pause segments (start_time, duration)\n",
        "- Visualization of audio waveform with highlighted pauses\n",
        "\n",
        "## Auxiliary Functions:\n",
        "\n",
        "1. SmoothSignal(signal, window_len):\n",
        "   window = ones(window_len) / window_len\n",
        "   RETURN convolve(signal, window, 'same')\n",
        "\n",
        "2. CalculateVarianceThreshold(samples, window_size):\n",
        "   FOR i in range(len(samples)):\n",
        "       start = max(0, i - window_size)\n",
        "       end = min(len(samples), i + window_size)\n",
        "       variance[i] = variance(samples[start:end])\n",
        "   RETURN SmoothSignal(variance, window_size)\n",
        "\n",
        "## Main Algorithm:\n",
        "\n",
        "1. Initialize:\n",
        "   samples = normalize(audio_to_array(audio_file))\n",
        "   IF stereo:\n",
        "       samples = convert_to_mono(samples)\n",
        "\n",
        "2. Calculate Envelope:\n",
        "   envelope = absolute_value(samples)\n",
        "   smoothed_envelope = SmoothSignal(envelope)\n",
        "   variance = CalculateVarianceThreshold(smoothed_envelope)\n",
        "   smoothed_variance = SmoothSignal(variance)\n",
        "\n",
        "3. Define Thresholds:\n",
        "   amplitude_threshold = mean(smoothed_envelope) × 0.1\n",
        "   variance_threshold = mean(smoothed_variance) × 0.1\n",
        "\n",
        "4. Detect Silent Regions:\n",
        "   silent_mask = (smoothed_envelope < amplitude_threshold) AND\n",
        "                (smoothed_variance < variance_threshold)\n",
        "\n",
        "5. Extract Pause Segments:\n",
        "   silent_segments = []\n",
        "   current_start = NULL\n",
        "   last_end = 0\n",
        "   \n",
        "   FOR i, is_silent in enumerate(silent_mask):\n",
        "       IF is_silent AND current_start is NULL:\n",
        "           current_start = i\n",
        "       ELSE IF not is_silent AND current_start exists:\n",
        "           duration = (i - current_start) × 1000 / sample_rate\n",
        "           IF duration ≥ min_silence_len:\n",
        "               start_time = current_start × 1000 / sample_rate\n",
        "               IF silent_segments not empty AND\n",
        "                  start_time ≤ last_end + debounce_time:\n",
        "                   Merge with previous segment\n",
        "               ELSE:\n",
        "                   Add new segment (start_time, duration)\n",
        "           current_start = NULL\n",
        "\n",
        "6. Visualization:\n",
        "   Plot waveform\n",
        "   FOR each pause in silent_segments:\n",
        "       Draw vertical line at pause start\n",
        "       Highlight pause duration region\n",
        "\n",
        "## Complexity:\n",
        "- Time: O(n), where n is the number of samples\n",
        "- Space: O(n) for storing smoothed signals and results\n",
        "\n",
        "## Constraints:\n",
        "- Minimum silence length: 500ms\n",
        "- Debounce time: 300ms\n",
        "- Amplitude threshold: 10% of mean envelope\n",
        "- Variance threshold: 10% of mean variance"
      ],
      "metadata": {
        "id": "n_YBHBRx1J5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm: Dominant Frequency Analysis of Audio Signal\n",
        "\n",
        "## Input:\n",
        "- audio_file: Input audio signal\n",
        "- sample_rate: Sampling frequency (Hz)\n",
        "- window_size: Size of analysis window (default: 1024 samples)\n",
        "\n",
        "## Output:\n",
        "- Time series of dominant frequencies\n",
        "- Visualization of frequency variation over time\n",
        "\n",
        "## Main Algorithm:\n",
        "\n",
        "1. Initialize:\n",
        "   step_size = window_size/2\n",
        "   frequencies = FFT_FREQUENCIES(window_size, sample_rate)\n",
        "   dominant_frequencies = []\n",
        "\n",
        "2. PreprocessAudio:\n",
        "   data = READ_AUDIO(audio_file)\n",
        "   IF data is stereo:\n",
        "       data = CONVERT_TO_MONO(data)\n",
        "\n",
        "3. DominantFrequencyAnalysis:\n",
        "   FOR i = 0 to length(data) - window_size STEP step_size:\n",
        "       # Extract and window the frame\n",
        "       frame = data[i : i + window_size]\n",
        "       windowed_frame = frame × HANNING_WINDOW(window_size)\n",
        "       \n",
        "       # Perform FFT\n",
        "       magnitude_spectrum = |FFT(windowed_frame)|\n",
        "       \n",
        "       # Find peak frequency\n",
        "       peak_index = ARGMAX(magnitude_spectrum)\n",
        "       dominant_freq = frequencies[peak_index]\n",
        "       \n",
        "       dominant_frequencies.APPEND(dominant_freq)\n",
        "\n",
        "4. TimeAxisGeneration:\n",
        "   time_values = [i × (window_size/2)/sample_rate\n",
        "                 for i in range(length(dominant_frequencies))]\n",
        "\n",
        "5. Visualization:\n",
        "   PLOT(time_values, dominant_frequencies)\n",
        "   SET_XLABEL(\"Time (seconds)\")\n",
        "   SET_YLABEL(\"Frequency (Hz)\")\n",
        "   SET_TITLE(\"Dominant Frequency Over Time\")\n",
        "\n",
        "## Parameters:\n",
        "- Window size: 1024 samples\n",
        "- Step size: 512 samples (50% overlap)\n",
        "- Window function: Hanning window\n",
        "\n",
        "## Complexity:\n",
        "- Time: O(N log N), where N is signal length\n",
        "- Space: O(N)\n",
        "\n",
        "## Mathematical Foundation:\n",
        "1. Hanning Window:\n",
        "   w(n) = 0.5(1 - cos(2πn/(N-1)))\n",
        "   where N is window length\n",
        "\n",
        "2. FFT Frequency Resolution:\n",
        "   f = k × (sample_rate/window_size)\n",
        "   where k is frequency bin index\n",
        "\n",
        "3. Frequency Range:\n",
        "   f_max = sample_rate/2 (Nyquist frequency)"
      ],
      "metadata": {
        "id": "m0bxkUqx1o9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm: Speech Characteristics Analysis and Classification\n",
        "\n",
        "## Input:\n",
        "- audio_file: Input audio file\n",
        "- word_timings: List of (timestamp, word) pairs\n",
        "- detected_pauses: List of (start_time, duration) for pauses\n",
        "\n",
        "## Output:\n",
        "- List of categorized speech characteristics per word\n",
        "\n",
        "## Auxiliary Functions:\n",
        "\n",
        "1. FindPauseDuration(start, end, pauses):\n",
        "    total_pause = 0\n",
        "    FOR each (pause_start, pause_length) in pauses:\n",
        "        pause_end = pause_start + pause_length\n",
        "        IF pause_start ∈ [start, end):\n",
        "            IF pause_end ≤ end:\n",
        "                total_pause += pause_length\n",
        "            ELSE:\n",
        "                total_pause += end - pause_start\n",
        "    RETURN total_pause\n",
        "\n",
        "## Main Algorithm:\n",
        "\n",
        "1. Initialize:\n",
        "   analysis_results = []\n",
        "   previous_end_time = 0\n",
        "\n",
        "2. Feature Extraction:\n",
        "   FOR i = 0 to length(word_timings) - 1:\n",
        "       current_word = word_timings[i]\n",
        "       next_word = word_timings[i + 1]\n",
        "       \n",
        "       # Time boundaries\n",
        "       start_time = max(current_word.time, previous_end_time)\n",
        "       end_time = next_word.time\n",
        "       \n",
        "       # Duration calculations\n",
        "       word_duration = end_time - start_time\n",
        "       pause_duration = FindPauseDuration(start_time, end_time, detected_pauses)\n",
        "       actual_duration = word_duration - pause_duration\n",
        "       \n",
        "       # Spectral analysis\n",
        "       audio_segment = ExtractAudioSegment(start_time, end_time)\n",
        "       IF ValidSegment(audio_segment):\n",
        "           samples = ConvertToArray(audio_segment)\n",
        "           spectrum = FFT(samples)\n",
        "           peak_frequency = FindPeakFrequency(spectrum)\n",
        "           peak_amplitude = max(|samples|)\n",
        "           \n",
        "           analysis_results.APPEND({\n",
        "               word: current_word.text,\n",
        "               amplitude: peak_amplitude,\n",
        "               duration: actual_duration,\n",
        "               pause: pause_duration,\n",
        "               frequency: peak_frequency\n",
        "           })\n",
        "       \n",
        "       previous_end_time = end_time\n",
        "\n",
        "3. Threshold Calculation:\n",
        "   amplitude_thresholds = PERCENTILE(amplitudes, [33, 66])\n",
        "   duration_thresholds = PERCENTILE(durations, [33, 66])\n",
        "   pause_threshold = PERCENTILE(pauses > 100ms, [50])\n",
        "   frequency_thresholds = PERCENTILE(frequencies, [33, 66])\n",
        "\n",
        "4. Classification:\n",
        "   categorized_results = []\n",
        "   FOR each entry in analysis_results:\n",
        "       categorized_entry = {\n",
        "           word: entry.word,\n",
        "           amplitude_category: CLASSIFY_AMPLITUDE(entry.amplitude, amplitude_thresholds),\n",
        "           duration_category: CLASSIFY_DURATION(entry.duration, duration_thresholds),\n",
        "           pause_category: CLASSIFY_PAUSE(entry.pause, pause_threshold),\n",
        "           frequency_category: CLASSIFY_FREQUENCY(entry.frequency, frequency_thresholds)\n",
        "       }\n",
        "       categorized_results.APPEND(categorized_entry)\n",
        "\n",
        "## Classification Rules:\n",
        "1. Amplitude Categories:\n",
        "   - Low: < 33rd percentile\n",
        "   - Medium: 33rd-66th percentile\n",
        "   - High: > 66th percentile\n",
        "\n",
        "2. Duration Categories:\n",
        "   - Short: < 33rd percentile\n",
        "   - Medium: 33rd-66th percentile\n",
        "   - Long: > 66th percentile\n",
        "\n",
        "3. Pause Categories:\n",
        "   - None: < 100ms\n",
        "   - Short: 100ms-median\n",
        "   - Long: > median\n",
        "\n",
        "4. Frequency Categories:\n",
        "   - Low: < 33rd percentile\n",
        "   - Medium: 33rd-66th percentile\n",
        "   - High: > 66th percentile\n",
        "\n",
        "## Complexity:\n",
        "- Time: O(n log n), where n is number of words\n",
        "- Space: O(n)"
      ],
      "metadata": {
        "id": "swVR1C8v2DgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm: Poetry Recitation Analysis System Using LLM\n",
        "\n",
        "## Input:\n",
        "- poem_content: Original poetry text\n",
        "- student_results: Student's recitation characteristics\n",
        "  {word, volume_level, duration_level, pause_level, pitch_level}\n",
        "- api_key: Authentication key for LLM service\n",
        "\n",
        "## Output:\n",
        "- Expert analysis of poetry recitation techniques\n",
        "- Personalized feedback on student's performance\n",
        "\n",
        "## Data Structures:\n",
        "1. ConversationHistory: Queue\n",
        "   - Elements: {role: String, content: String}\n",
        "2. Cache: LRU Cache\n",
        "   - Key: Message tuple\n",
        "   - Value: API response\n",
        "   - Size: 128 entries\n",
        "\n",
        "## Main Algorithm:\n",
        "\n",
        "1. Initialize:\n",
        "   conversation_history = EMPTY_QUEUE()\n",
        "   cache = LRU_CACHE(size=128)\n",
        "   llm_client = INITIALIZE_LLM_CLIENT(api_key)\n",
        "\n",
        "2. GetPoetryAnalysis(poem_content):\n",
        "    prompt = CONSTRUCT_PROMPT(\n",
        "        template: \"分析古诗朗读技巧：\n",
        "                  - 声音重点\n",
        "                  - 停顿位置\n",
        "                  - 节奏控制\n",
        "                  - 语调变化\",\n",
        "        content: poem_content\n",
        "    )\n",
        "    \n",
        "    ADD_TO_HISTORY(\"user\", prompt)\n",
        "    response = GetCachedResponse(conversation_history)\n",
        "    RETURN response\n",
        "\n",
        "3. GetStudentFeedback(student_results):\n",
        "    prompt = CONSTRUCT_PROMPT(\n",
        "        template: \"分析学生朗读表现：\n",
        "                  - 声量分析 (高/中/低)\n",
        "                  - 发声时长 (长/中/短)\n",
        "                  - 停顿控制 (长/短/无)\n",
        "                  - 音高变化 (高/中/低)\",\n",
        "        content: student_results\n",
        "    )\n",
        "    \n",
        "    ADD_TO_HISTORY(\"user\", prompt)\n",
        "    response = GetCachedResponse(conversation_history)\n",
        "    RETURN response\n",
        "\n",
        "4. GetCachedResponse(messages):\n",
        "    messages_tuple = CONVERT_TO_TUPLE(messages)\n",
        "    IF messages_tuple IN cache:\n",
        "        RETURN cache[messages_tuple]\n",
        "    ELSE:\n",
        "        response = llm_client.REQUEST(\n",
        "            model=\"glm-4\",\n",
        "            messages=messages\n",
        "        )\n",
        "        cache[messages_tuple] = response\n",
        "        RETURN response\n",
        "\n",
        "## Auxiliary Functions:\n",
        "\n",
        "1. AddToHistory(role, content):\n",
        "    conversation_history.APPEND({\n",
        "        role: role,\n",
        "        content: content\n",
        "    })\n",
        "\n",
        "2. ConvertToTuple(messages):\n",
        "    RETURN TUPLE(\n",
        "        FOR each message IN messages:\n",
        "            TUPLE(message.items())\n",
        "    )\n",
        "\n",
        "## System Features:\n",
        "1. Caching Mechanism:\n",
        "   - LRU cache implementation\n",
        "   - Prevents redundant API calls\n",
        "   - Optimizes response time\n",
        "\n",
        "2. Analysis Components:\n",
        "   - Poetry recitation techniques\n",
        "   - Volume analysis\n",
        "   - Timing and rhythm\n",
        "   - Pause placement\n",
        "   - Pitch variation\n",
        "\n",
        "## Complexity:\n",
        "- Time: O(1) for cached responses\n",
        "- Time: O(n) for API calls, where n is response length\n",
        "- Space: O(k) where k is cache size (128 entries)"
      ],
      "metadata": {
        "id": "hk8FpjsD2UMV"
      }
    }
  ]
}