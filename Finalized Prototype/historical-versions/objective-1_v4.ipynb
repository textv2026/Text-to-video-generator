{"cells":[{"cell_type":"markdown","source":["# Chinese Classical Poetry Visualization System\n","This system generates visual interpretations of Chinese classical poems using AI models, combining modified SDXL for image generation and GLM-4 for poem analysis."],"metadata":{"id":"IqIylVKzVznC"}},{"cell_type":"markdown","source":["## Setup and Dependencies\n","The following cell installs required packages and imports necessary libraries."],"metadata":{"id":"WT0eCbZVV3xV"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mg3bHU-NPZ3_","executionInfo":{"status":"ok","timestamp":1732755062869,"user_tz":-480,"elapsed":2297,"user":{"displayName":"cxch","userId":"08855671813663366935"}},"outputId":"4bdf6f97-221d-4460-e25f-61698628d430"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Required pip installations\n","!pip install transformers\n","!pip install diffusers\n","!pip install accelerate\n","!pip install zhipuai\n","!pip install moviepy\n","!pip install bayesian-optimization\n","!pip install xformers\n","!pip install safetensors\n","!pip install triton\n","\n","# Imports\n","import os\n","import gc\n","import json\n","import time\n","import torch\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","from PIL import Image, ImageDraw, ImageFont\n","from moviepy.editor import ImageClip, concatenate_videoclips\n","import traceback\n","import torch.nn.functional as F\n","from transformers import CLIPProcessor, CLIPModel\n","from diffusers import DiffusionPipeline, EulerDiscreteScheduler\n","from bayes_opt import BayesianOptimization\n","from zhipuai import ZhipuAI\n","from concurrent.futures import ThreadPoolExecutor\n","import re\n","import triton\n","import subprocess\n","\n","# Disable warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Enable cuda if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A84llWIjOozv","executionInfo":{"status":"ok","timestamp":1732755090112,"user_tz":-480,"elapsed":27247,"user":{"displayName":"cxch","userId":"08855671813663366935"}},"outputId":"c53c324d-519a-4732-a0d6-1b7d417e15a3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.31.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.16.1)\n","Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.26.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.26.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.5)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (11.0.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (2024.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.8.30)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n","Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.26.2)\n","Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n","Requirement already satisfied: zhipuai in /usr/local/lib/python3.10/dist-packages (2.1.5.20230904)\n","Requirement already satisfied: cachetools>=4.2.2 in /usr/local/lib/python3.10/dist-packages (from zhipuai) (5.5.0)\n","Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from zhipuai) (0.27.2)\n","Requirement already satisfied: pydantic<3.0,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from zhipuai) (2.9.2)\n","Requirement already satisfied: pydantic-core>=2.14.6 in /usr/local/lib/python3.10/dist-packages (from zhipuai) (2.23.4)\n","Requirement already satisfied: pyjwt<2.9.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from zhipuai) (2.8.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai) (3.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.23.0->zhipuai) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0,>=1.9.0->zhipuai) (0.7.0)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0,>=1.9.0->zhipuai) (4.12.2)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.0->zhipuai) (1.2.2)\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n","Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.36.0)\n","Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.5.1)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.6)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.26.4)\n","Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.32.3)\n","Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n","Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio_ffmpeg>=0.2.0->moviepy) (75.1.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.8.30)\n","Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.10/dist-packages (2.0.0)\n","Requirement already satisfied: colorama<0.5.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (0.4.6)\n","Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.26.4)\n","Requirement already satisfied: scikit-learn<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.5.2)\n","Requirement already satisfied: scipy<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian-optimization) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian-optimization) (3.5.0)\n","Requirement already satisfied: xformers in /usr/local/lib/python3.10/dist-packages (0.0.28.post3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.26.4)\n","Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.5.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->xformers) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->xformers) (3.0.2)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.4.5)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:xformers:A matching Triton is not available, some optimizations will not be enabled\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/xformers/__init__.py\", line 57, in _is_triton_available\n","    import triton  # noqa\n","ModuleNotFoundError: No module named 'triton'\n"]}]},{"cell_type":"markdown","source":["## Main System Implementation\n","This cell contains the complete implementation including:\n","- Utility functions (font management, GPU memory, Drive operations)\n","- PoemAnalyzer class (GLM-4 based poem analysis and ontology)\n","- BayesianStableDiffusion class (Modified SDXL image generation)\n","- ModelComparisonExperiment class (experiment management)\n","- Main execution flow\n","\n","Key Features:\n","- Automated poem analysis and understanding\n","- High-quality image generation with refinement\n","- CLIP-guided image selection\n","- Video generation with text overlays\n","- Performance optimization and reporting\n","\n","Usage:\n","1. Run the cell\n","2. Select a poem when prompted\n","3. Wait for processing (3-5 minutes per line)\n","4. View generated images and video\n","5. Check performance metrics"],"metadata":{"id":"ZFAgeDa-WHP3"}},{"cell_type":"code","source":["def find_available_font():\n","    \"\"\"Find an available font for text rendering.\"\"\"\n","    font_paths = [\n","        \"/usr/share/fonts/truetype/noto/NotoSansCJK-Bold.ttc\",\n","        \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\",\n","        \"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\"\n","    ]\n","\n","    for path in font_paths:\n","        try:\n","            if os.path.exists(path):\n","                return path\n","        except:\n","            pass\n","    return None\n","\n","def clear_gpu_memory():\n","    \"\"\"Clear GPU memory and cache.\"\"\"\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    if torch.cuda.is_available():\n","        torch.cuda.synchronize()\n","\n","def save_to_drive(video_path):\n","    \"\"\"Save the output to Google Drive if mounted\"\"\"\n","    from google.colab import drive\n","    try:\n","        drive.mount('/content/drive')\n","        import shutil\n","        drive_path = f\"/content/drive/MyDrive/Colab Notebooks/Capstone/Video Generated/{os.path.basename(video_path)}\"\n","        os.makedirs(os.path.dirname(drive_path), exist_ok=True)\n","        shutil.copy(video_path, drive_path)\n","        print(f\"Video saved to Drive: {drive_path}\")\n","    except Exception as e:\n","        print(f\"Could not save to Drive: {e}\")\n","\n","def load_poem_from_json(json_file_path, poem_title):\n","    \"\"\"Load a specific poem from the JSON file.\"\"\"\n","    try:\n","        with open(json_file_path, 'r', encoding='utf-8') as f:\n","            data = json.load(f)\n","\n","        for poem in data['poems']:\n","            if poem['title'] == poem_title:\n","                return poem\n","\n","        print(f\"Poem '{poem_title}' not found in the database.\")\n","        return None\n","\n","    except Exception as e:\n","        print(f\"Error loading poem data: {str(e)}\")\n","        return None\n","\n","# Define models to compare\n","MODELS_TO_COMPARE = {\n","    \"SDXL\": \"stabilityai/stable-diffusion-xl-base-1.0\"\n","}\n","\n","class EnhancedPoemAnalyzer:\n","    def __init__(self, api_key=\"968cd0b672b9b5133d01741721558a95.xYFanKwaJ2ShpQuZ\"):\n","        self.client = ZhipuAI(api_key=api_key)\n","        self.chunk_cache = {}\n","\n","    def translate_to_english(self, chinese_text):\n","        prompt = f\"\"\"Please translate the following Chinese text to English, maintaining the visual and descriptive nature of the content:\n","\n","        Chinese text:\n","        {chinese_text}\n","\n","        Requirements:\n","        1. Translate to natural, fluent English\n","        2. Preserve all visual descriptions and imagery\n","        3. Keep any technical or specific terms\n","        4. Maintain the original structure where applicable\n","        \"\"\"\n","\n","        response = self.client.chat.completions.create(\n","            model=\"glm-4\",\n","            messages=[{\"role\": \"user\", \"content\": prompt}]\n","        )\n","\n","        return response.choices[0].message.content.strip()\n","\n","    def interpret_cultural_terms(self, text, context):\n","        prompt = f\"\"\"请分析这句诗中的专有名词或文化意象，将其转换为具体的视觉描述：\n","\n","        原诗上下文：\n","        {context}\n","\n","        需要分析的句子：\n","        {text}\n","\n","        请识别所有的专有名词、人物称谓或文化意象，并给出具体的视觉描述。\n","        格式要求：\n","        1. 每个词一行\n","        2. 用 \"词：视觉描述\" 的格式\n","        3. 描述必须是具体的、可视化的，避免抽象概念\n","        4. 描述要符合诗歌语境\n","        5. 请直接用英文描述\n","\n","        示例：\n","        若\"王孙\"在此诗中表达隐居的文人，应描述为\"a scholarly man in traditional robes meditating in nature\"\n","        若\"渔父\"出现，应描述为\"an old fisherman in simple clothes on a wooden boat\"\n","        \"\"\"\n","\n","        response = self.client.chat.completions.create(\n","            model=\"glm-4\",\n","            messages=[{\"role\": \"user\", \"content\": prompt}]\n","        )\n","\n","        interpretations = {}\n","        for line in response.choices[0].message.content.strip().split('\\n'):\n","            if '：' in line:\n","                term, desc = line.split('：', 1)\n","                interpretations[term.strip()] = desc.strip()\n","\n","        interpreted_text = text\n","        for term, desc in interpretations.items():\n","            interpreted_text = interpreted_text.replace(term, desc)\n","\n","        return interpreted_text\n","\n","    def get_poem_understanding(self, full_poem):\n","        study_prompt = f\"\"\"请从视觉角度分析这首诗的整体意境、场景和情感，重点描述：\n","        1. 主要场景和环境特征\n","        2. 光线和时间的变化\n","        3. 人物的动作和状态\n","        4. 整体氛围和情感基调\n","\n","        诗文：\n","        {full_poem}\n","\n","        请用英文回答，使用具体的视觉语言描述，避免抽象概念。\"\"\"\n","\n","        response = self.client.chat.completions.create(\n","            model=\"glm-4\",\n","            messages=[{\"role\": \"user\", \"content\": study_prompt}]\n","        )\n","\n","        return response.choices[0].message.content\n","\n","    def analyze_chunk_detail(self, chunk, category, context_dict):\n","        cache_key = f\"{chunk}_{category}\"\n","        if cache_key in self.chunk_cache:\n","            return self.chunk_cache[cache_key]\n","\n","        previous_chunk = context_dict.get('previous_chunk', '')\n","        interpreted_chunk = self.interpret_cultural_terms(chunk, context_dict['full_poem'])\n","\n","        prompts = {\n","            \"subject_action\": f\"\"\"\n","            Analyze the subjects (people/animal/living beings) and their actions in this line/segment of poetry.\n","            Original: \"{chunk}\"\n","            Previous line: \"{previous_chunk}\"\n","            Interpreted: \"{interpreted_chunk}\"\n","\n","            Return in this exact format:\n","            subjects: [concrete description of each person/animal/living being, their clothing/appearance if mentioned]\n","            actions: [specific descriptions of actions]\n","\n","            Requirements:\n","            - The output must be consistent with and referred to the poem understanding\n","            - For subjects, always include full description\n","            - If no explicit subject in current line, use the subject from previous line\n","            - If previous line has subject \"孤鸿\" (lonely goose), current line should use \"the lonely goose\" as subject\n","            - All actions must be visually concrete (e.g., for \"不敢顾\", show \"lifting its wings away from\")\n","            - Avoid literary descriptions and Chinese terms\n","            - If this is the first line ({context_dict['chunk_index'] == 0}), only include subjects explicitly mentioned\n","            - If subject is not further described in terms of appearance or clothing in this or previous line, do not mention \"not further describe\" or \"not explicitly stated\" in final output\n","            - Do not put explanations or interpretations in brackets\n","            - List each subject and action exactly once\n","            - Use concise, visual descriptions\n","            \"\"\",\n","\n","            \"scene_setting\": f\"\"\"\n","            Analyze the scene and environmental elements in this line:\n","            Original: \"{chunk}\"\n","            Interpreted: \"{interpreted_chunk}\"\n","\n","            Please return in this format:\n","            locations: [specific scene locations]\n","            objects: [specific physical objects, flora, fauna, celestial elements and architectural elements]\n","            Requirements:\n","            - The output must be consistent with and referred to the poem understanding\n","            - Descriptions must be concrete and visual\n","            - Avoid display of Chinese in final output\n","            - All physical elements must be included! (e.g. 举杯邀明月 will have cup and moon; 巢在三珠树 will have nests on branches, three pearl trees)\n","            - Take note of the amounts too (e.g. three birds)\n","            - Be extremely specific about objects (e.g., \"three pearl trees\" rather than just \"trees\")\n","            - No explanations in the brackets\n","            - List each element exactly once\n","            - Use concise, visual terms\n","            \"\"\",\n","\n","            \"time_weather\": f\"\"\"\n","            Analyze the time and weather elements in this line:\n","            Original: \"{chunk}\"\n","            Interpreted: \"{interpreted_chunk}\"\n","\n","            Please return in this format:\n","            time: [specific time, e.g., sunset, dawn]\n","            weather: [specific weather conditions]\n","            Requirements:\n","            - The output must be consistent with and referred to the poem understanding\n","            - Use commonly recognized natural phenomena\n","            - Avoid display of Chinese in final output\n","            - If there is a moon mentioned, set time to night\n","            - No explanations in the brackets\n","            - List each element exactly once\n","            - Use concise, visual terms\n","            \"\"\",\n","\n","            \"mood\": f\"\"\"\n","            Analyze the visual atmosphere and emotional elements in this line:\n","            Original: \"{chunk}\"\n","            Interpreted: \"{interpreted_chunk}\"\n","\n","            Please return in this format:\n","            lighting: [specific lighting effects]\n","            atmosphere: [specific visual mood]\n","            color_tone: [main color tones]\n","            Requirements:\n","            - The output must be consistent with and referred to the poem understanding\n","            - All descriptions should be directly usable for image generation\n","            - Avoid display of Chinese in final output\n","            - No explanations in the brackets\n","            - List each element exactly once\n","            - Use concise, visual terms\n","            \"\"\"\n","        }\n","\n","        response = self.client.chat.completions.create(\n","            model=\"glm-4\",\n","            messages=[{\"role\": \"user\", \"content\": prompts[category]}]\n","        )\n","\n","        result = response.choices[0].message.content.strip()\n","        self.chunk_cache[cache_key] = result\n","        return result\n","\n","    def analyze_chunk_parallel(self, chunk, context):\n","        with ThreadPoolExecutor(max_workers=4) as executor:\n","            futures = {\n","                \"subject_action\": executor.submit(self.analyze_chunk_detail, chunk, \"subject_action\", context),\n","                \"scene_setting\": executor.submit(self.analyze_chunk_detail, chunk, \"scene_setting\", context),\n","                \"time_weather\": executor.submit(self.analyze_chunk_detail, chunk, \"time_weather\", context),\n","                \"mood\": executor.submit(self.analyze_chunk_detail, chunk, \"mood\", context)\n","            }\n","\n","            return {\n","                \"text\": chunk,\n","                \"subject_action\": futures[\"subject_action\"].result(),\n","                \"scene_setting\": futures[\"scene_setting\"].result(),\n","                \"time_weather\": futures[\"time_weather\"].result(),\n","                \"mood\": futures[\"mood\"].result()\n","            }\n","\n","    def pack_chunk_to_prompt(self, chunk_analysis, overall_understanding):\n","        try:\n","            elements = {\n","                'primary_subjects': [],\n","                'secondary_subjects': [],\n","                'actions': [],\n","                'objects': [],\n","                'environment': [],\n","                'lighting': [],\n","                'atmosphere': [],\n","                'style': ['RTX ultra detailed realism', 'traditional China']\n","            }\n","\n","            # Extract subjects and their relationships\n","            if 'subject_action' in chunk_analysis:\n","                content = chunk_analysis['subject_action']\n","                if 'subjects: [' in content and 'actions: [' in content:\n","                    subjects = content.split('subjects: [')[1].split(']')[0].split(', ')\n","                    actions = content.split('actions: [')[1].split(']')[0].split(', ')\n","\n","                    # Separate primary and secondary subjects\n","                    for subject in subjects:\n","                        if 'hunter' in subject.lower():\n","                            elements['secondary_subjects'].append(subject)\n","                        else:\n","                            elements['primary_subjects'].append(subject)\n","                    elements['actions'].extend(actions)\n","\n","            # Build the prompt with proper relationships\n","            prompt_parts = []\n","\n","            # Combine subjects and actions\n","            if elements['primary_subjects'] and elements['secondary_subjects']:\n","                subjects_str = f\"{', '.join(elements['primary_subjects'])}, {', '.join(elements['secondary_subjects'])}\"\n","                prompt_parts.append(f\"featuring {subjects_str}\")\n","            elif elements['primary_subjects']:\n","                prompt_parts.append(f\"featuring {', '.join(elements['primary_subjects'])}\")\n","\n","            if elements['actions']:\n","                prompt_parts.append(f\"with {', '.join(elements['actions'])}\")\n","\n","            # Add remaining elements\n","            if elements['objects']:\n","                prompt_parts.append(f\"including {', '.join(elements['objects'])}\")\n","            if elements['environment']:\n","                prompt_parts.append(f\"in {', '.join(elements['environment'])}\")\n","            if elements['lighting']:\n","                prompt_parts.append(f\"with {', '.join(elements['lighting'])}\")\n","            if elements['atmosphere']:\n","                prompt_parts.append(f\"creating {', '.join(elements['atmosphere'])} atmosphere\")\n","\n","            prompt_parts.append(', '.join(elements['style']))\n","\n","            return ' | '.join(filter(None, prompt_parts))\n","\n","        except Exception as e:\n","            print(f\"Error in pack_chunk_to_prompt: {e}\")\n","            return \"Traditional Chinese landscape painting in elegant style\"\n","\n","    def get_contextual_analysis(self, full_poem, segment, context_dict):\n","        overall_understanding = self.get_poem_understanding(full_poem)\n","        detailed_analysis = self.analyze_chunk_parallel(segment, context_dict)\n","        compact_prompt = self.pack_chunk_to_prompt(detailed_analysis, overall_understanding)\n","\n","        translated_poem = self.translate_to_english(full_poem)\n","\n","        return {\n","            \"original_poem\": full_poem,\n","            \"translated_poem\": translated_poem,\n","            \"overall_understanding\": overall_understanding,\n","            \"detailed_analysis\": detailed_analysis,\n","            \"compact_prompt\": compact_prompt\n","        }\n","\n","class BayesianStableDiffusion:\n","    def __init__(self, model_id=\"stabilityai/stable-diffusion-xl-base-1.0\", num_inference_steps=50,\n","                 clip_model_name=\"openai/clip-vit-base-patch32\"):\n","        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        self.model_id = model_id\n","        self.refiner_id = \"stabilityai/stable-diffusion-xl-refiner-1.0\"\n","\n","        print(f\"Initializing models on device: {self.device}\")\n","        if torch.cuda.is_available():\n","            print(f\"Available CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n","            clear_gpu_memory()\n","\n","        try:\n","            # Load base model\n","            print(f\"Loading base model {model_id}...\")\n","            self.base = DiffusionPipeline.from_pretrained(\n","                model_id,\n","                torch_dtype=torch.float16,\n","                variant=\"fp16\",\n","                use_safetensors=True\n","            ).to(self.device)\n","\n","            # Load refiner model\n","            print(f\"Loading refiner model...\")\n","            self.refiner = DiffusionPipeline.from_pretrained(\n","                self.refiner_id,\n","                torch_dtype=torch.float16,\n","                variant=\"fp16\",\n","                use_safetensors=True,\n","                text_encoder_2=self.base.text_encoder_2,\n","                vae=self.base.vae,\n","            ).to(self.device)\n","\n","            # Configure schedulers\n","            self.base.scheduler = EulerDiscreteScheduler.from_config(\n","                self.base.scheduler.config,\n","                use_karras_sigmas=True\n","            )\n","            self.refiner.scheduler = EulerDiscreteScheduler.from_config(\n","                self.refiner.scheduler.config,\n","                use_karras_sigmas=True\n","            )\n","\n","            # Enable optimizations for both models\n","            for pipe in [self.base, self.refiner]:\n","                try:\n","                    pipe.enable_attention_slicing(slice_size=\"auto\")\n","                    pipe.enable_vae_slicing()\n","                    pipe.enable_xformers_memory_efficient_attention()\n","                except Exception as e:\n","                    print(f\"Warning: Could not enable some optimizations: {e}\")\n","\n","            print(\"Loading CLIP model...\")\n","            self.num_inference_steps = num_inference_steps\n","            self.clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n","            self.clip_model = CLIPModel.from_pretrained(clip_model_name).to(self.device)\n","            self.clip_model.eval()\n","\n","            print(\"Model initialization completed\")\n","\n","        except Exception as e:\n","            print(f\"Error initializing model: {str(e)}\")\n","            traceback.print_exc()\n","            raise\n","\n","    def generate_images(self, prompt, negative_prompt=\"\", num_samples=5, guidance_scale=7.5, temperature=1.0):\n","        try:\n","            clear_gpu_memory()\n","\n","            print(f\"Generating {num_samples} images with prompt: {prompt}\")\n","\n","            # First pass with base model\n","            base_images = self.base(\n","                prompt=[prompt] * num_samples,\n","                negative_prompt=[negative_prompt] * num_samples,\n","                num_inference_steps=30,\n","                denoising_end=0.8,\n","                guidance_scale=guidance_scale,\n","                width=1024,\n","                height=1024,\n","            ).images\n","\n","            # Second pass with refiner\n","            refined_images = []\n","            for base_image in base_images:\n","                refined = self.refiner(\n","                    prompt=prompt,\n","                    negative_prompt=negative_prompt,\n","                    image=base_image,\n","                    num_inference_steps=20,\n","                    denoising_start=0.8,\n","                    guidance_scale=guidance_scale,\n","                ).images[0]\n","                refined_images.append(refined)\n","\n","            if not refined_images:\n","                raise ValueError(\"No images were generated\")\n","\n","            # Ensure all images are in RGB mode\n","            images = [img.convert('RGB') if isinstance(img, Image.Image) else Image.fromarray(img).convert('RGB')\n","                    for img in refined_images]\n","\n","            # Compute CLIP scores\n","            likelihoods = self.compute_clip_likelihoods(images, prompt)\n","\n","            clear_gpu_memory()\n","            return images, likelihoods\n","\n","        except Exception as e:\n","            print(f\"Error in generate_images: {str(e)}\")\n","            traceback.print_exc()\n","            return [], np.array([])\n","\n","    def compute_clip_likelihoods(self, images, prompt):\n","        try:\n","            inputs = self.clip_processor(\n","                text=[prompt] * len(images),\n","                images=images,\n","                return_tensors=\"pt\",\n","                padding=True\n","            ).to(self.device)\n","\n","            with torch.no_grad():\n","                outputs = self.clip_model(**inputs)\n","                image_embeds = F.normalize(outputs.image_embeds, p=2, dim=1)\n","                text_embeds = F.normalize(outputs.text_embeds, p=2, dim=1)\n","                cosine_similarity = F.cosine_similarity(image_embeds, text_embeds, dim=1)\n","                likelihoods = (cosine_similarity + 1) / 2\n","            return likelihoods.cpu().numpy()\n","\n","        except Exception as e:\n","            print(f\"Error in compute_clip_likelihoods: {str(e)}\")\n","            traceback.print_exc()\n","            return np.array([0.0] * len(images))\n","\n","    def compute_mean_and_variance(self, images):\n","        if isinstance(images[0], Image.Image):\n","            images = [np.array(img) for img in images]\n","        images_array = np.array(images) / 255.0\n","        mean_image = np.mean(images_array, axis=0)\n","        variance_image = np.var(images_array, axis=0)\n","        return mean_image, variance_image\n","\n","class ModelComparisonExperiment:\n","    def __init__(self):\n","        self.models = {}\n","        self.results = {}\n","        self.best_images_sequence = {}\n","        self.poem_analyzer = EnhancedPoemAnalyzer()\n","        self.load_models()\n","\n","    def load_models(self):\n","        for model_name, model_id in MODELS_TO_COMPARE.items():\n","            print(f\"Loading {model_name}...\")\n","            try:\n","                self.models[model_name] = BayesianStableDiffusion(\n","                    model_id=model_id,\n","                    num_inference_steps=50\n","                )\n","                print(f\"Successfully loaded {model_name}\")\n","            except Exception as e:\n","                print(f\"Error loading {model_name}: {str(e)}\")\n","\n","    def run_comparison(self, poem):\n","        print(\"\\nStarting poem analysis...\")\n","\n","        # Get overall poem understanding first\n","        overall_analysis = self.poem_analyzer.get_poem_understanding(poem)\n","\n","        # Split poem into chunks and keep track of their order\n","        chunks = [chunk.strip() for chunk in re.split('[，。？！]', poem) if chunk.strip()]\n","\n","        results = {\n","            model_name: {\n","                'images': [],\n","                'scores': [],\n","                'generation_times': [],\n","                'clip_scores': [],\n","                'optimization_results': [],\n","                'best_images': []\n","            } for model_name in self.models.keys()\n","        }\n","\n","        # Create context dictionary for each chunk\n","        chunk_contexts = {}\n","        for i, chunk in enumerate(chunks):\n","            chunk_contexts[chunk] = {\n","                'full_poem': poem,\n","                'previous_chunk': chunks[i-1] if i > 0 else None,\n","                'chunk_index': i\n","            }\n","\n","        for chunk in tqdm(chunks, desc=\"Processing chunks\"):\n","            print(f\"\\nProcessing chunk: {chunk}\")\n","\n","            # Pass the context dictionary to get_contextual_analysis\n","            chunk_analysis = self.poem_analyzer.get_contextual_analysis(\n","                poem,\n","                chunk,\n","                chunk_contexts[chunk]\n","            )\n","\n","\n","            for model_name, model in self.models.items():\n","                print(f\"\\nUsing model: {model_name}\")\n","\n","                try:\n","                    start_time = time.time()\n","\n","                    # Use the enhanced prompt generation\n","                    main_prompt = chunk_analysis[\"compact_prompt\"]\n","                    negative_prompt = \"low quality, blurry, bad anatomy, bad composition, deformed\"\n","\n","                    print(f\"Generated prompt: {main_prompt}\")\n","\n","                    optimal_scale = optimize_guidance_scale(\n","                        model,\n","                        main_prompt,\n","                        negative_prompt,\n","                        num_samples=5\n","                    )\n","\n","                    images, likelihoods = model.generate_images(\n","                        main_prompt,\n","                        negative_prompt=negative_prompt,\n","                        num_samples=5,\n","                        guidance_scale=optimal_scale\n","                    )\n","\n","                    if images and len(images) > 0 and len(likelihoods) > 0:\n","                        generation_time = time.time() - start_time\n","                        best_idx = np.argmax(likelihoods)\n","                        best_image = images[best_idx]\n","\n","                        results[model_name]['best_images'].append({\n","                            'image': best_image,\n","                            'text': chunk,\n","                            'prompt': main_prompt,\n","                            'likelihood': likelihoods[best_idx],\n","                            'analysis': chunk_analysis\n","                        })\n","\n","                        results[model_name]['images'].append(images[best_idx])\n","                        results[model_name]['scores'].append(likelihoods[best_idx])\n","                        results[model_name]['generation_times'].append(generation_time)\n","                        results[model_name]['clip_scores'].append(np.mean(likelihoods))\n","                        results[model_name]['optimization_results'].append(optimal_scale)\n","\n","                        self.display_model_comparison(\n","                            images,\n","                            likelihoods,\n","                            model_name,\n","                            main_prompt,\n","                            generation_time,\n","                            optimal_scale,\n","                            chunk_analysis\n","                        )\n","                    else:\n","                        print(f\"No valid images generated for {model_name}\")\n","\n","                except Exception as e:\n","                    print(f\"Error processing chunk with {model_name}: {str(e)}\")\n","                    traceback.print_exc()\n","                    continue\n","\n","        self.results = results\n","        return results\n","\n","    def display_model_comparison(self, images, likelihoods, model_name, prompt, generation_time, guidance_scale, analysis):\n","        mean_image, variance_image = self.models[model_name].compute_mean_and_variance(images)\n","\n","        n = len(images) + 2\n","        fig = plt.figure(figsize=(5*n, 12))\n","        gs = gridspec.GridSpec(4, n, height_ratios=[1, 1, 8, 1])\n","\n","        prompt_ax = plt.subplot(gs[1, :])\n","        prompt_ax.axis('off')\n","        prompt_ax.text(0.5, 0.5, f\"Model: {model_name}\\nPrompt: {prompt}\",\n","                      ha='center', va='center', wrap=True,\n","                      fontsize=12)\n","\n","        axes = [plt.subplot(gs[2, i]) for i in range(n)]\n","        best_idx = np.argmax(likelihoods)\n","\n","        for i, (ax, img) in enumerate(zip(axes[:len(images)], images)):\n","            ax.imshow(img)\n","            ax.axis('off')\n","\n","            if i == best_idx:\n","                title = f\"Selected Image\\nLikelihood: {likelihoods[i]:.3f}\"\n","                ax.set_title(title, color='green', fontweight='bold')\n","            else:\n","                title = f\"Sample {i+1}\\nLikelihood: {likelihoods[i]:.3f}\"\n","                ax.set_title(title)\n","\n","        axes[-2].imshow(mean_image)\n","        axes[-2].axis('off')\n","        axes[-2].set_title(\"Mean Image\")\n","\n","        axes[-1].imshow(variance_image, cmap='viridis')\n","        axes[-1].axis('off')\n","        axes[-1].set_title(\"Variance Image\")\n","\n","        metrics_ax = plt.subplot(gs[3, :])\n","        metrics_ax.axis('off')\n","        metrics_text = f\"Generation Time: {generation_time:.2f}s | \"\n","        metrics_text += f\"Mean CLIP Score: {np.mean(likelihoods):.3f} | \"\n","        metrics_text += f\"Optimal Guidance Scale: {guidance_scale:.2f}\"\n","        metrics_ax.text(0.5, 0.5, metrics_text,\n","                       ha='center', va='center',\n","                       fontsize=10)\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","def optimize_guidance_scale(model, prompt, negative_prompt=\"\", num_samples=5):\n","    def objective(guidance_scale):\n","        try:\n","            images, likelihoods = model.generate_images(\n","                prompt,\n","                negative_prompt=negative_prompt,\n","                num_samples=num_samples,\n","                guidance_scale=guidance_scale\n","            )\n","            return np.mean(likelihoods) if len(likelihoods) > 0 else 0.0\n","        except Exception as e:\n","            print(f\"Error in objective function: {str(e)}\")\n","            return 0.0\n","\n","    try:\n","        optimizer = BayesianOptimization(\n","            f=objective,\n","            pbounds={\"guidance_scale\": (7.0, 12.0)},\n","            random_state=42,\n","            verbose=0\n","        )\n","\n","        optimizer.maximize(\n","            init_points=2,\n","            n_iter=5\n","        )\n","        return optimizer.max['params']['guidance_scale']\n","    except Exception as e:\n","        print(f\"Error in optimization: {str(e)}\")\n","        return 7.5\n","\n","def main():\n","    # Load available poems and let user choose\n","    json_file_path = '/content/drive/MyDrive/Colab Notebooks/Capstone/Poem Database/poem_database.json'  # Replace with your JSON file path\n","    try:\n","        with open(json_file_path, 'r', encoding='utf-8') as f:\n","            data = json.load(f)\n","            print(\"\\nAvailable poems:\")\n","            for poem in data['poems']:\n","                print(f\"- {poem['title']}\")\n","    except Exception as e:\n","        print(f\"Error loading poems file: {e}\")\n","        return\n","\n","    # Get user input\n","    poem_title = input(\"\\nWhat poem would you like to visualize? \")\n","\n","    # Load the selected poem\n","    poem_data = load_poem_from_json(json_file_path, poem_title)\n","    if not poem_data:\n","        print(\"Failed to load poem data.\")\n","        return\n","\n","    poem = poem_data['content']\n","    print(f\"\\nLoaded poem: {poem_data['title']}\")\n","    print(f\"Author: {poem_data['author']}\")\n","    print(f\"Content: {poem}\")\n","\n","    # Create and run the experiment\n","    experiment = ModelComparisonExperiment()\n","    results = experiment.run_comparison(poem)\n","\n","    # Generate visualization video with poem data\n","    # experiment.create_visualization_video(poem, poem_data)  # Pass both poem content and metadata\n","\n","    # Create report\n","    report = pd.DataFrame({\n","        model_name: {\n","            'Mean CLIP Score': np.mean(data['clip_scores']) if data['clip_scores'] else 0.0,\n","            'Mean Generation Time': np.mean(data['generation_times']) if data['generation_times'] else 0.0,\n","            'Mean Optimal Scale': np.mean(data['optimization_results']) if data['optimization_results'] else 0.0,\n","            'Best Score': max(data['scores']) if data['scores'] else 0.0,\n","            'Worst Score': min(data['scores']) if data['scores'] else 0.0\n","        }\n","        for model_name, data in results.items()\n","    }).T\n","\n","    print(\"\\nModel Comparison Report:\")\n","    print(report)\n","\n","    plt.figure(figsize=(15, 5))\n","    metrics = ['Mean CLIP Score', 'Mean Generation Time', 'Mean Optimal Scale']\n","    for i, metric in enumerate(metrics, 1):\n","        plt.subplot(1, 3, i)\n","        report[metric].plot(kind='bar')\n","        plt.title(metric)\n","        plt.xticks(rotation=45)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["339370ffc4694a15b1ae1ed9f41b9eb1","1a8154919f1e4b86bf7a801d53f7e47b","7ee2aaeae0164e6c9f281f1b5b3050a5","10dcf345824941e3b51cec3a05a7e947","e00ef51f249844f5b4095eca7c759adc","3cc97bc8eca848f989e55089f7390c1e","86b96f0f9d4e420fa3fdb18fee20c9cd","513d9b849e7d4cab8f682ad9fb324192","50aab527113e4b81bccb37f7b94a3146","6aee77b5c24d4a78a15772d20ab8cce4","ad27d4f0d5274811b620fd09de3d6d54","c760b9502fab4edb89d477390155dcf2","eaffc52c021943f6807c5d6f4c82788f","f42470f235864cc080ae807eb0e456b1","819e4331226144f3a6df345eabae3393","29565357e0054008830d320d9fda707b","f9b8a4f2aa05492c946e375f1790c597","92ae0a563bd84096a29fa12cb3ba73ef","e4f56796ad8a423ca8ae509a647d8764","dd329662d1b2440d8f31061c6bde1857","a0a4b29c6b54462fae84553f70913ccb","08c6b472baab4e1db1faa86520106bf4","3e02839c9624450cbda92e53693b34e9","59e72d3df44144459303c5f5073d1050","bf3aaa753e1c41a39bb9d32f8a3e9e4b","b7a29d2b0a67495a8465963a4c171433","5f2ab8008737496fb23e7579b6db1826","bc1c5beeb7154543a8318d60162610d0","5d9316cbf29744838f8f43f93959931a","5ac1b50c175b4359835796cd3436e850","03a6e2751ef14952b2ddafc30d73bf94","485ac52449b844ae961cf7709cb83ff9","2ed4c247c27840f8b25c0a194a555ac7"]},"id":"aTQ4CfRNl8HS","executionInfo":{"status":"error","timestamp":1732768007746,"user_tz":-480,"elapsed":90972,"user":{"displayName":"cxch","userId":"08855671813663366935"}},"outputId":"3646eb01-4485-4b94-b9f1-ffc2488b731c"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Available poems:\n","- 感遇四首其一\n","- 感遇四首其二\n","- 感遇四首其三\n","- 感遇四首其四\n","- 下终南山过斛斯山人宿置酒\n","- 月下独酌\n","- 春思\n","- 望岳\n","- 赠卫八处士\n","- 佳人\n","- 梦李白二首·其一\n","- 梦李白二首·其二\n","- 送别\n","- 送綦毋潜落第还乡\n","- 青溪\n","- 渭川田家\n","- 西施咏\n","- 同从弟南斋玩月忆山阴崔少府\n","- 郡斋雨中与诸文士燕集\n","- 初发扬子寄元大校书\n","- 寄全椒山中道士\n","- 长安遇冯著\n","- 夕次盱眙县\n","- 东郊\n","- 送杨氏女\n","- 晨诣超师院读禅经\n","- 溪居\n","- 塞上曲·其一\n","- 塞下曲\n","- 关山月\n","- 子夜四时歌·春歌\n","- 子夜四时歌·夏歌\n","- 子夜四时歌·秋歌\n","- 子夜四时歌·冬歌\n","- 烈女操\n","- 游子吟\n","- 登幽州台歌\n","- 古意\n","- 送陈章甫\n","- 琴歌\n","- 听董大弹胡笳弄兼寄语房给事\n","- 听安万善吹觱篥歌\n","- 夜归鹿门山歌\n","- 庐山谣寄卢侍御虚舟\n","- 梦游天姥吟留别 \n","- 金陵酒肆留别 \n","\n","What poem would you like to visualize? 感遇四首其一\n","\n","Loaded poem: 感遇四首其一\n","Author: 张九龄\n","Content: 孤鸿海上来，池潢不敢顾。侧见双翠鸟，巢在三珠树。矫矫珍木巅，得无金丸惧？美服患人指，高明逼神恶？今我游冥冥，弋者何所慕！\n","Loading SDXL...\n","Initializing models on device: cuda\n","Available CUDA memory: 42.48 GB\n","Loading base model stabilityai/stable-diffusion-xl-base-1.0...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"339370ffc4694a15b1ae1ed9f41b9eb1"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loading refiner model...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c760b9502fab4edb89d477390155dcf2"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loading CLIP model...\n","Model initialization completed\n","Successfully loaded SDXL\n","\n","Starting poem analysis...\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing chunks:   0%|          | 0/10 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Processing chunk: 孤鸿海上来\n","\n","Using model: SDXL\n","Generated prompt: featuring the lonely goose | with coming from across the sea | RTX ultra detailed realism, traditional China\n","Generating 5 images with prompt: featuring the lonely goose | with coming from across the sea | RTX ultra detailed realism, traditional China\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/19 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e02839c9624450cbda92e53693b34e9"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\rProcessing chunks:   0%|          | 0/10 [00:49<?, ?it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-5e290d8d9f71>\u001b[0m in \u001b[0;36m<cell line: 738>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-5e290d8d9f71>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    705\u001b[0m     \u001b[0;31m# Create and run the experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0mexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelComparisonExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_comparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m     \u001b[0;31m# Generate visualization video with poem data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-5e290d8d9f71>\u001b[0m in \u001b[0;36mrun_comparison\u001b[0;34m(self, poem)\u001b[0m\n\u001b[1;32m    545\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Generated prompt: {main_prompt}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m                     optimal_scale = optimize_guidance_scale(\n\u001b[0m\u001b[1;32m    548\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m                         \u001b[0mmain_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-5e290d8d9f71>\u001b[0m in \u001b[0;36moptimize_guidance_scale\u001b[0;34m(model, prompt, negative_prompt, num_samples)\u001b[0m\n\u001b[1;32m    667\u001b[0m         )\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m         optimizer.maximize(\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mx_probe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"No target function has been provided.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-5e290d8d9f71>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(guidance_scale)\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguidance_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m             images, likelihoods = model.generate_images(\n\u001b[0m\u001b[1;32m    651\u001b[0m                 \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                 \u001b[0mnegative_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-5e290d8d9f71>\u001b[0m in \u001b[0;36mgenerate_images\u001b[0;34m(self, prompt, negative_prompt, num_samples, guidance_scale, temperature)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;31m# First pass with base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             base_images = self.base(\n\u001b[0m\u001b[1;32m    405\u001b[0m                 \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m                 \u001b[0mnegative_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnegative_prompt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, prompt, prompt_2, height, width, num_inference_steps, timesteps, sigmas, denoising_end, guidance_scale, negative_prompt, negative_prompt_2, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, guidance_rescale, original_size, crops_coords_top_left, target_size, negative_original_size, negative_crops_coords_top_left, negative_target_size, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1209\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mip_adapter_image\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mip_adapter_image_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m                     \u001b[0madded_cond_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_embeds\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m                 noise_pred = self.unet(\n\u001b[0m\u001b[1;32m   1212\u001b[0m                     \u001b[0mlatent_model_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/unets/unet_2d_condition.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupsample_block\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"has_cross_attention\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mupsample_block\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_cross_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m                 sample = upsample_block(\n\u001b[0m\u001b[1;32m   1282\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m                     \u001b[0mtemb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/unets/unet_2d_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, res_hidden_states_tuple, temb, encoder_hidden_states, cross_attention_kwargs, upsample_size, attention_mask, encoder_attention_mask)\u001b[0m\n\u001b[1;32m   2549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m                 hidden_states = attn(\n\u001b[0m\u001b[1;32m   2552\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/transformers/transformer_2d.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, added_cond_kwargs, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 )\n\u001b[1;32m    441\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m                 hidden_states = block(\n\u001b[0m\u001b[1;32m    443\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels, added_cond_kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0mgligen_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_attention_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gligen\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         attn_output = self.attn1(\n\u001b[0m\u001b[1;32m    508\u001b[0m             \u001b[0mnorm_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monly_cross_attention\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/attention_processor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0mcross_attention_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcross_attention_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattn_parameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         return self.processor(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/attention_processor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, temb, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2187\u001b[0m         \u001b[0;31m# linear proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2188\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2189\u001b[0m         \u001b[0;31m# dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2190\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["## Results and Output\n","The system will:\n","- Display generated images for each poem segment\n","- Save a visualization video to your Drive\n","- Show performance metrics and comparison charts\n","\n","Video output location:\n","`/content/drive/MyDrive/Colab Notebooks/Capstone/Video Generated/`"],"metadata":{"id":"lOLR9jXrWZ5v"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"1mgeabMvThFva9eRbhUCIa3wPFfrnfVyf","timestamp":1732753658528}],"authorship_tag":"ABX9TyOtRgC+SndK+4htaLMqveYj"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"339370ffc4694a15b1ae1ed9f41b9eb1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1a8154919f1e4b86bf7a801d53f7e47b","IPY_MODEL_7ee2aaeae0164e6c9f281f1b5b3050a5","IPY_MODEL_10dcf345824941e3b51cec3a05a7e947"],"layout":"IPY_MODEL_e00ef51f249844f5b4095eca7c759adc"}},"1a8154919f1e4b86bf7a801d53f7e47b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3cc97bc8eca848f989e55089f7390c1e","placeholder":"​","style":"IPY_MODEL_86b96f0f9d4e420fa3fdb18fee20c9cd","value":"Loading pipeline components...: 100%"}},"7ee2aaeae0164e6c9f281f1b5b3050a5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_513d9b849e7d4cab8f682ad9fb324192","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50aab527113e4b81bccb37f7b94a3146","value":7}},"10dcf345824941e3b51cec3a05a7e947":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6aee77b5c24d4a78a15772d20ab8cce4","placeholder":"​","style":"IPY_MODEL_ad27d4f0d5274811b620fd09de3d6d54","value":" 7/7 [00:01&lt;00:00,  3.41it/s]"}},"e00ef51f249844f5b4095eca7c759adc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cc97bc8eca848f989e55089f7390c1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86b96f0f9d4e420fa3fdb18fee20c9cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"513d9b849e7d4cab8f682ad9fb324192":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50aab527113e4b81bccb37f7b94a3146":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6aee77b5c24d4a78a15772d20ab8cce4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad27d4f0d5274811b620fd09de3d6d54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c760b9502fab4edb89d477390155dcf2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eaffc52c021943f6807c5d6f4c82788f","IPY_MODEL_f42470f235864cc080ae807eb0e456b1","IPY_MODEL_819e4331226144f3a6df345eabae3393"],"layout":"IPY_MODEL_29565357e0054008830d320d9fda707b"}},"eaffc52c021943f6807c5d6f4c82788f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9b8a4f2aa05492c946e375f1790c597","placeholder":"​","style":"IPY_MODEL_92ae0a563bd84096a29fa12cb3ba73ef","value":"Loading pipeline components...: 100%"}},"f42470f235864cc080ae807eb0e456b1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4f56796ad8a423ca8ae509a647d8764","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dd329662d1b2440d8f31061c6bde1857","value":5}},"819e4331226144f3a6df345eabae3393":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0a4b29c6b54462fae84553f70913ccb","placeholder":"​","style":"IPY_MODEL_08c6b472baab4e1db1faa86520106bf4","value":" 5/5 [00:00&lt;00:00,  3.15it/s]"}},"29565357e0054008830d320d9fda707b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9b8a4f2aa05492c946e375f1790c597":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92ae0a563bd84096a29fa12cb3ba73ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e4f56796ad8a423ca8ae509a647d8764":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd329662d1b2440d8f31061c6bde1857":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a0a4b29c6b54462fae84553f70913ccb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08c6b472baab4e1db1faa86520106bf4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e02839c9624450cbda92e53693b34e9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_59e72d3df44144459303c5f5073d1050","IPY_MODEL_bf3aaa753e1c41a39bb9d32f8a3e9e4b","IPY_MODEL_b7a29d2b0a67495a8465963a4c171433"],"layout":"IPY_MODEL_5f2ab8008737496fb23e7579b6db1826"}},"59e72d3df44144459303c5f5073d1050":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc1c5beeb7154543a8318d60162610d0","placeholder":"​","style":"IPY_MODEL_5d9316cbf29744838f8f43f93959931a","value":" 32%"}},"bf3aaa753e1c41a39bb9d32f8a3e9e4b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ac1b50c175b4359835796cd3436e850","max":19,"min":0,"orientation":"horizontal","style":"IPY_MODEL_03a6e2751ef14952b2ddafc30d73bf94","value":6}},"b7a29d2b0a67495a8465963a4c171433":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_485ac52449b844ae961cf7709cb83ff9","placeholder":"​","style":"IPY_MODEL_2ed4c247c27840f8b25c0a194a555ac7","value":" 6/19 [00:03&lt;00:06,  2.05it/s]"}},"5f2ab8008737496fb23e7579b6db1826":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc1c5beeb7154543a8318d60162610d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d9316cbf29744838f8f43f93959931a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ac1b50c175b4359835796cd3436e850":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03a6e2751ef14952b2ddafc30d73bf94":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"485ac52449b844ae961cf7709cb83ff9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ed4c247c27840f8b25c0a194a555ac7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}